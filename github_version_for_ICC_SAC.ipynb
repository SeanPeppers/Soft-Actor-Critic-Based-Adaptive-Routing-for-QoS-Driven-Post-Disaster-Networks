{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1PbgkZE2sm4"
      },
      "outputs": [],
      "source": [
        "!pip install optuna\n",
        "!pip install sympy==1.12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnT84yNx4SQU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np\n",
        "import random\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from collections import deque\n",
        "import time\n",
        "\n",
        "network_topology = {\n",
        "    0: [1, 2], 1: [0, 3], 2: [0, 4], 3: [1, 4], 4: [2, 3]\n",
        "}\n",
        "\n",
        "class RoutingEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    A stochastic network routing environment where packet success is determined\n",
        "    by target node congestion and bottleneck bandwidth.\n",
        "    \"\"\"\n",
        "    def __init__(self, total_nodes=5, max_ttl=10, max_congestion=5, max_bandwidth=10,\n",
        "                 base_link_success_rate=0.9, base_link_latency=1.0):\n",
        "        super().__init__()\n",
        "        self.total_nodes = total_nodes\n",
        "        self.max_ttl = max_ttl\n",
        "        self.max_congestion = max_congestion\n",
        "        self.max_bandwidth = max_bandwidth\n",
        "        self.base_link_success_rate = base_link_success_rate\n",
        "        self.base_link_latency = base_link_latency\n",
        "        self.network_topology = network_topology\n",
        "\n",
        "        self.max_neighbors_sense = 3\n",
        "        state_size = 2 * self.total_nodes + 3 + self.max_neighbors_sense\n",
        "\n",
        "        self.observation_space = spaces.Box(low=0, high=1, shape=(state_size,), dtype=np.float32)\n",
        "        self.action_space = spaces.Discrete(self.total_nodes + 1)\n",
        "\n",
        "        self.current_node = None\n",
        "        self.destination_node = None\n",
        "        self.ttl_remaining = None\n",
        "        self.node_congestions = {}\n",
        "        self.node_bandwidths = {}\n",
        "        self.time_window_deadline = 0.0\n",
        "        self.total_steps = 0\n",
        "        self.accumulated_latency = 0.0\n",
        "        self.min_path_bandwidth = float('inf')\n",
        "        self.successful_hop_delays = []\n",
        "        self.successful_hops_count = 0\n",
        "\n",
        "        self.np_random, _ = gym.utils.seeding.np_random()\n",
        "\n",
        "    def get_state_features(self, current_node, dest_node, ttl, congestions, bandwidths):\n",
        "        curr_vec = np.zeros(self.total_nodes); curr_vec[current_node] = 1\n",
        "        dest_vec = np.zeros(self.total_nodes); dest_vec[dest_node] = 1\n",
        "\n",
        "        ttl_norm = ttl / self.max_ttl\n",
        "        local_cong = congestions[current_node] / self.max_congestion\n",
        "        local_bw = bandwidths[current_node] / self.max_bandwidth\n",
        "\n",
        "        neighbors = self.network_topology.get(current_node, [])\n",
        "        neighbor_cong_features = []\n",
        "\n",
        "        for i in range(self.max_neighbors_sense):\n",
        "            if i < len(neighbors):\n",
        "                n_id = neighbors[i]\n",
        "                neighbor_cong_features.append(congestions[n_id] / self.max_congestion)\n",
        "            else:\n",
        "                neighbor_cong_features.append(0.0)\n",
        "\n",
        "        state_vector = np.concatenate([\n",
        "            curr_vec, dest_vec, [ttl_norm],\n",
        "            [local_cong], [local_bw],\n",
        "            neighbor_cong_features\n",
        "        ])\n",
        "        return state_vector.astype(np.float32)\n",
        "\n",
        "    def get_action_space(self, current_node_id):\n",
        "        return self.network_topology.get(current_node_id, []) + ['drop']\n",
        "\n",
        "    def get_reward(self, current_node, destination_node, action, link_successful, is_terminal=False, terminal_reason=None):\n",
        "        if action == 'drop':\n",
        "            return -20\n",
        "        elif is_terminal:\n",
        "            if terminal_reason == \"Goal Reached\": return 100\n",
        "            if terminal_reason == \"Time Window Deadline Missed\": return -50\n",
        "            if terminal_reason == \"TTL Expired\": return -10\n",
        "        return -1 if link_successful else -20\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.current_node = self.np_random.integers(0, self.total_nodes)\n",
        "        self.destination_node = self.np_random.integers(0, self.total_nodes)\n",
        "        while self.destination_node == self.current_node:\n",
        "             self.destination_node = self.np_random.integers(0, self.total_nodes)\n",
        "\n",
        "        self.ttl_remaining = self.np_random.integers(max(1, self.max_ttl // 2), self.max_ttl + 1)\n",
        "\n",
        "        for node in range(self.total_nodes):\n",
        "            self.node_bandwidths[node] = self.np_random.uniform(0.7 * self.max_bandwidth, self.max_bandwidth)\n",
        "            self.node_congestions[node] = self.np_random.uniform(0, self.max_congestion)\n",
        "\n",
        "        min_time_estimate = 3 * self.base_link_latency\n",
        "        self.time_window_deadline = self.np_random.uniform(1.2 * min_time_estimate, 2.0 * min_time_estimate)\n",
        "\n",
        "        self.total_steps = 0\n",
        "        self.accumulated_latency = 0.0\n",
        "        self.min_path_bandwidth = float('inf')\n",
        "        self.successful_hop_delays = []\n",
        "        self.successful_hops_count = 0\n",
        "\n",
        "        return self.get_state_features(\n",
        "            self.current_node, self.destination_node, self.ttl_remaining,\n",
        "            self.node_congestions, self.node_bandwidths\n",
        "        ), {\"deadline\": self.time_window_deadline}\n",
        "\n",
        "    def step(self, action_index):\n",
        "        available_actions = self.get_action_space(self.current_node)\n",
        "        self.total_steps += 1\n",
        "\n",
        "        if action_index >= len(available_actions): action = 'drop'\n",
        "        else: action = available_actions[action_index]\n",
        "\n",
        "        initial_node = self.current_node\n",
        "        reward, done, link_successful = 0, False, False\n",
        "        next_node = self.current_node\n",
        "        terminal_reason = None\n",
        "\n",
        "        # Update global network dynamics\n",
        "        for node in range(self.total_nodes):\n",
        "             replenish = self.np_random.uniform(-0.1 * self.max_bandwidth, 0.5 * self.max_bandwidth)\n",
        "             self.node_bandwidths[node] = max(0, min(self.max_bandwidth, self.node_bandwidths[node] + replenish))\n",
        "             delta = self.np_random.uniform(-1.0, 1.0)\n",
        "             self.node_congestions[node] = max(0, min(self.max_congestion, self.node_congestions[node] + delta))\n",
        "\n",
        "        if action == 'drop':\n",
        "            terminal_reason = \"Dropped by Agent\"\n",
        "            done = True\n",
        "        else:\n",
        "            neighbor_node = action\n",
        "            target_cong = self.node_congestions[neighbor_node]\n",
        "            target_bw = self.node_bandwidths[neighbor_node]\n",
        "\n",
        "            cong_norm = target_cong / self.max_congestion\n",
        "            bw_norm = target_bw / self.max_bandwidth\n",
        "\n",
        "            qos_penalty = (cong_norm * 0.4) + ((1 - bw_norm) * 0.2)\n",
        "            effective_success_rate = max(0.05, self.base_link_success_rate - qos_penalty)\n",
        "\n",
        "            if self.np_random.random() < effective_success_rate:\n",
        "                link_successful = True\n",
        "                next_node = neighbor_node\n",
        "                self.successful_hops_count += 1\n",
        "                self.min_path_bandwidth = min(self.min_path_bandwidth, target_bw)\n",
        "                current_delay = self.base_link_latency * (1.0 + (cong_norm * 2.0))\n",
        "\n",
        "                self.node_bandwidths[neighbor_node] = max(0, self.node_bandwidths[neighbor_node] - self.np_random.uniform(0.1, 0.5))\n",
        "                self.node_congestions[neighbor_node] = min(self.max_congestion, self.node_congestions[neighbor_node] + 0.5)\n",
        "            else:\n",
        "                current_delay = self.base_link_latency * 2.0\n",
        "\n",
        "            self.accumulated_latency += current_delay\n",
        "            if link_successful: self.successful_hop_delays.append(current_delay)\n",
        "            self.ttl_remaining -= 1\n",
        "\n",
        "            if self.accumulated_latency > self.time_window_deadline:\n",
        "                terminal_reason = \"Time Window Deadline Missed\"\n",
        "                done = True\n",
        "            elif self.ttl_remaining <= 0:\n",
        "                terminal_reason = \"TTL Expired\"\n",
        "                done = True\n",
        "            elif next_node == self.destination_node:\n",
        "                terminal_reason = \"Goal Reached\"\n",
        "                done = True\n",
        "\n",
        "            if not done:\n",
        "                reward = self.get_reward(next_node, self.destination_node, action, link_successful)\n",
        "                self.current_node = next_node\n",
        "            else:\n",
        "                reward = self.get_reward(initial_node, self.destination_node, action, link_successful, is_terminal=True, terminal_reason=terminal_reason)\n",
        "\n",
        "        info = {}\n",
        "        if done:\n",
        "            info = {\n",
        "                \"terminal_reason\": terminal_reason,\n",
        "                \"episode_success\": (terminal_reason == \"Goal Reached\"),\n",
        "                \"total_steps\": self.total_steps,\n",
        "                \"successful_hops\": self.successful_hops_count,\n",
        "                \"rtt_latency\": self.accumulated_latency,\n",
        "                \"throughput_bottleneck\": self.min_path_bandwidth if terminal_reason == \"Goal Reached\" else 0.0,\n",
        "                \"throughput_proxy\": (1.0 / self.accumulated_latency) if self.accumulated_latency > 0 else 0.0,\n",
        "                \"jitter_std_dev\": np.std(self.successful_hop_delays) if len(self.successful_hop_delays) > 1 else 0.0\n",
        "            }\n",
        "\n",
        "        return self.get_state_features(self.current_node, self.destination_node, self.ttl_remaining, self.node_congestions, self.node_bandwidths), reward, done, False, info\n",
        "\n",
        "    def render(self): pass\n",
        "    def close(self): pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Owfg-Ecz5BXU"
      },
      "outputs": [],
      "source": [
        "# --- SAC AGENT AND NETWORK DEFINITIONS ---\n",
        "\n",
        "# State size: 2 * total_nodes + 3 (TTL, Congestion, Bandwidth) -> 13\n",
        "INPUT_DIMS = 13\n",
        "# Action size: total_nodes + 1 (Drop action) -> 6\n",
        "N_ACTIONS = 6\n",
        "# Increased dimensions to match the improved network architecture\n",
        "FC_DIMS = 512\n",
        "\n",
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, input_dims=INPUT_DIMS, n_actions=N_ACTIONS, fc1_dims=FC_DIMS, fc2_dims=FC_DIMS):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dims, fc1_dims)\n",
        "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
        "        self.fc3 = nn.Linear(fc2_dims, n_actions)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        logits = self.fc3(x)\n",
        "        return logits\n",
        "\n",
        "class DuelingCriticNetwork(nn.Module):\n",
        "    def __init__(self, input_dims=INPUT_DIMS, n_actions=N_ACTIONS, fc1_dims=FC_DIMS, fc2_dims=FC_DIMS):\n",
        "        super(DuelingCriticNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dims, fc1_dims)\n",
        "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
        "        self.V = nn.Linear(fc2_dims, 1)\n",
        "        self.A = nn.Linear(fc2_dims, n_actions)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        V = self.V(x)\n",
        "        A = self.A(x)\n",
        "        Q_values = V + (A - A.mean(dim=1, keepdim=True))\n",
        "        return Q_values\n",
        "\n",
        "class CombinedDuelingCritic(nn.Module):\n",
        "    def __init__(self, input_dims=INPUT_DIMS, n_actions=N_ACTIONS, fc1_dims=FC_DIMS, fc2_dims=FC_DIMS):\n",
        "        super(CombinedDuelingCritic, self).__init__()\n",
        "        self.q1 = DuelingCriticNetwork(input_dims, n_actions, fc1_dims, fc2_dims)\n",
        "        self.q2 = DuelingCriticNetwork(input_dims, n_actions, fc1_dims, fc2_dims)\n",
        "\n",
        "    def forward(self, state):\n",
        "        q1_values = self.q1(state)\n",
        "        q2_values = self.q2(state)\n",
        "        return q1_values, q2_values\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Replay buffer using collections.deque for efficient fixed-capacity management.\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity):\n",
        "        # Use deque for automatic fixed-size management\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def add(self, transition):\n",
        "        # deque automatically handles capacity limit\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # We rely on SACAgent.learn() to check buffer size before calling sample\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "class SACAgent:\n",
        "    def __init__(self, input_dims=INPUT_DIMS, n_actions=N_ACTIONS, alpha=0.2, tau=0.005, gamma=0.99, lr=3e-4,\n",
        "                 buffer_capacity=1_000_000, batch_size=256, fc1_dims=FC_DIMS, fc2_dims=FC_DIMS):\n",
        "\n",
        "        self.gamma = gamma; self.tau = tau; self.lr = lr; self.batch_size = batch_size; self.n_actions = n_actions\n",
        "\n",
        "        # The network constructors now receive fc1_dims=512 and fc2_dims=128\n",
        "        self.actor = ActorNetwork(input_dims, n_actions, fc1_dims, fc2_dims)\n",
        "        self.critic = CombinedDuelingCritic(input_dims, n_actions, fc1_dims, fc2_dims)\n",
        "        self.target_critic = CombinedDuelingCritic(input_dims, n_actions, fc1_dims, fc2_dims)\n",
        "\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
        "        self.critic_1_optimizer = optim.Adam(self.critic.q1.parameters(), lr=lr)\n",
        "        self.critic_2_optimizer = optim.Adam(self.critic.q2.parameters(), lr=lr)\n",
        "\n",
        "        self.update_target_networks(tau=1.0)\n",
        "\n",
        "        self.target_entropy = -torch.log(torch.tensor(1.0 / self.n_actions))\n",
        "        self.log_alpha = torch.tensor(np.log(alpha), requires_grad=True)\n",
        "        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=lr)\n",
        "        self.alpha = self.log_alpha.exp()\n",
        "\n",
        "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
        "\n",
        "    def update_target_networks(self, tau=None):\n",
        "        if tau is None: tau = self.tau\n",
        "        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
        "            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
        "\n",
        "    def choose_action(self, observation, evaluate=False):\n",
        "        state = torch.tensor([observation], dtype=torch.float)\n",
        "        logits = self.actor(state)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        if evaluate: action = torch.argmax(probs).item()\n",
        "        else:\n",
        "            dist = Categorical(probs=probs)\n",
        "            action = dist.sample().item()\n",
        "        return action\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.replay_buffer) < self.batch_size: return\n",
        "\n",
        "        transitions = self.replay_buffer.sample(self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*transitions)\n",
        "\n",
        "        states = torch.tensor(np.array(states), dtype=torch.float)\n",
        "        actions = torch.tensor(np.array(actions), dtype=torch.long).unsqueeze(-1)\n",
        "        rewards = torch.tensor(np.array(rewards), dtype=torch.float).unsqueeze(-1)\n",
        "        next_states = torch.tensor(np.array(next_states), dtype=torch.float)\n",
        "        dones = torch.tensor(np.array(dones), dtype=torch.float).unsqueeze(-1)\n",
        "\n",
        "        # Critic updates\n",
        "        with torch.no_grad():\n",
        "            next_logits = self.actor(next_states)\n",
        "            # Standardize calculation of policy output vectors\n",
        "            next_probs = F.softmax(next_logits, dim=-1)\n",
        "            next_log_probs = F.log_softmax(next_logits, dim=-1) # Log Probs for V-value\n",
        "\n",
        "            target_q1_next, target_q2_next = self.target_critic(next_states)\n",
        "            min_target_q_next = torch.min(target_q1_next, target_q2_next)\n",
        "\n",
        "            # Calculate V(s') = sum(pi(a|s') * (Q(s', a) - alpha * log pi(a|s')))\n",
        "            next_v_value = torch.sum(next_probs * (min_target_q_next - self.alpha.detach() * next_log_probs), dim=1, keepdim=True)\n",
        "            target_q_value = rewards + (1 - dones) * self.gamma * next_v_value\n",
        "\n",
        "        q1_current, q2_current = self.critic(states)\n",
        "        q1_taken_action = q1_current.gather(1, actions)\n",
        "        q2_taken_action = q2_current.gather(1, actions)\n",
        "\n",
        "        critic_loss_1 = F.mse_loss(q1_taken_action, target_q_value)\n",
        "        critic_loss_2 = F.mse_loss(q2_taken_action, target_q_value)\n",
        "        critic_loss = critic_loss_1 + critic_loss_2\n",
        "\n",
        "        self.critic_1_optimizer.zero_grad(); self.critic_2_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_1_optimizer.step(); self.critic_2_optimizer.step()\n",
        "\n",
        "        # Actor updates\n",
        "        logits = self.actor(states)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q1_current, q2_current = self.critic(states)\n",
        "            min_q_current = torch.min(q1_current, q2_current)\n",
        "\n",
        "        policy_loss = torch.sum(probs * (self.alpha.detach() * log_probs - min_q_current), dim=1).mean()\n",
        "\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # --- Alpha (entropy) updates ---\n",
        "        # FIX: Correctly calculate the expected log probability (negative entropy)\n",
        "        # We sum (probs * log_probs) to get the expectation E[log pi(a|s)]\n",
        "        log_prob_expected = torch.sum(probs * log_probs, dim=1, keepdim=True)\n",
        "        alpha_loss = -(self.log_alpha * (log_prob_expected + self.target_entropy).detach()).mean()\n",
        "\n",
        "        self.alpha_optimizer.zero_grad()\n",
        "        alpha_loss.backward()\n",
        "        self.alpha_optimizer.step()\n",
        "        self.alpha = self.log_alpha.exp()\n",
        "\n",
        "        self.update_target_networks()\n",
        "        return policy_loss.item(), critic_loss.item(), alpha_loss.item()\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.replay_buffer.add((state, action, reward, next_state, done))\n",
        "\n",
        "    def save_models(self, path):\n",
        "        # Save combined critics\n",
        "        torch.save(self.actor.state_dict(), f\"{path}_actor.pth\")\n",
        "        torch.save(self.critic.state_dict(), f\"{path}_critic.pth\")\n",
        "        torch.save(self.target_critic.state_dict(), f\"{path}_target_critic.pth\")\n",
        "        # Save optimizers and alpha\n",
        "        torch.save(self.actor_optimizer.state_dict(), f\"{path}_actor_optimizer.pth\")\n",
        "        torch.save(self.critic_1_optimizer.state_dict(), f\"{path}_critic1_optimizer.pth\")\n",
        "        torch.save(self.critic_2_optimizer.state_dict(), f\"{path}_critic2_optimizer.pth\")\n",
        "        torch.save(self.alpha_optimizer.state_dict(), f\"{path}_alpha_optimizer.pth\")\n",
        "        torch.save(self.log_alpha, f\"{path}_log_alpha.pt\")\n",
        "\n",
        "    def load_models(self, path):\n",
        "        # Load combined critics\n",
        "        self.actor.load_state_dict(torch.load(f\"{path}_actor.pth\"))\n",
        "        self.critic.load_state_dict(torch.load(f\"{path}_critic.pth\"))\n",
        "        self.target_critic.load_state_dict(torch.load(f\"{path}_target_critic.pth\"))\n",
        "        # Load optimizers and alpha\n",
        "        self.actor_optimizer.load_state_dict(torch.load(f\"{path}_actor_optimizer.pth\"))\n",
        "        self.critic_1_optimizer.load_state_dict(torch.load(f\"{path}_critic1_optimizer.pth\"))\n",
        "        self.critic_2_optimizer.load_state_dict(torch.load(f\"{path}_critic2_optimizer.pth\"))\n",
        "        self.alpha_optimizer.load_state_dict(torch.load(f\"{path}_alpha_optimizer.pth\"))\n",
        "        self.log_alpha = torch.load(f\"{path}_log_alpha.pt\")\n",
        "        self.alpha = self.log_alpha.exp()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdxqdtxP25JA"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def objective(trial):\n",
        "    \"\"\"\n",
        "    Optuna objective function to optimize SAC agent hyperparameters.\n",
        "    Trains for 1000 episodes and returns the average reward of 50 evaluation episodes.\n",
        "    \"\"\"\n",
        "    # Hyperparameter Search Space\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
        "    tau = trial.suggest_float(\"tau\", 0.001, 0.01)\n",
        "    alpha = trial.suggest_float(\"alpha\", 0.01, 0.5)\n",
        "    gamma = trial.suggest_float(\"gamma\", 0.9, 0.999)\n",
        "    buffer_capacity = trial.suggest_int(\"buffer_capacity\", 10_000, 1_000_000, step=10_000)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512])\n",
        "    fc1_dims = trial.suggest_categorical(\"fc1_dims\", [128, 256, 512])\n",
        "    fc2_dims = trial.suggest_categorical(\"fc2_dims\", [128, 256, 512])\n",
        "\n",
        "    env = RoutingEnv()\n",
        "    input_dims = env.observation_space.shape[0]\n",
        "    n_actions = env.action_space.n\n",
        "\n",
        "    agent = SACAgent(\n",
        "        input_dims=input_dims, n_actions=n_actions,\n",
        "        lr=lr, tau=tau, alpha=alpha, gamma=gamma,\n",
        "        buffer_capacity=buffer_capacity, batch_size=batch_size,\n",
        "        fc1_dims=fc1_dims, fc2_dims=fc2_dims\n",
        "    )\n",
        "\n",
        "    # Training Phase\n",
        "    num_training_episodes = 1000\n",
        "    max_steps = 100\n",
        "\n",
        "    print(f\"Trial {trial.number}: Starting training with params: {trial.params}\")\n",
        "\n",
        "    for episode in range(num_training_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        step_count = 0\n",
        "\n",
        "        while not done and step_count < max_steps:\n",
        "            action = agent.choose_action(state, evaluate=False)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.store_transition(state, action, reward, next_state, done)\n",
        "\n",
        "            if len(agent.replay_buffer) > agent.batch_size:\n",
        "                 agent.learn()\n",
        "\n",
        "            state = next_state\n",
        "            step_count += 1\n",
        "\n",
        "    # Evaluation Phase\n",
        "    num_eval_episodes = 50\n",
        "    eval_rewards = []\n",
        "\n",
        "    print(f\"Trial {trial.number}: Starting evaluation...\")\n",
        "\n",
        "    for _ in range(num_eval_episodes):\n",
        "        eval_state, _ = env.reset()\n",
        "        eval_done = False\n",
        "        eval_episode_reward = 0\n",
        "        eval_step_count = 0\n",
        "\n",
        "        while not eval_done and eval_step_count < max_steps:\n",
        "            eval_action = agent.choose_action(eval_state, evaluate=True)\n",
        "            eval_next_state, eval_reward, term, trunc, _ = env.step(eval_action)\n",
        "            eval_done = term or trunc\n",
        "            eval_state = eval_next_state\n",
        "            eval_episode_reward += eval_reward\n",
        "            eval_step_count += 1\n",
        "\n",
        "        eval_rewards.append(eval_episode_reward)\n",
        "\n",
        "    avg_eval_reward = np.mean(eval_rewards)\n",
        "    print(f\"Trial {trial.number} Finished. Avg Reward: {avg_eval_reward:.2f}\")\n",
        "\n",
        "    return avg_eval_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OK1q8UUT283p"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "\n",
        "# Run the optimization for a specified number of trials.\n",
        "n_trials = 50\n",
        "print(\"Starting Optuna study...\")\n",
        "study.optimize(objective, n_trials=n_trials)\n",
        "print(\"Optuna study finished.\")\n",
        "\n",
        "# Print the best trial's information.\n",
        "print(\"\\nBest trial:\")\n",
        "trial = study.best_trial\n",
        "print(f\"  Value: {trial.value}\")\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z89TE-TS2_CU"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "# --- TRAINING CONFIGURATION ---\n",
        "num_episodes = 10000\n",
        "max_steps = 100\n",
        "report_interval = 50\n",
        "eval_interval = 100\n",
        "num_eval_episodes = 20\n",
        "best_model_path = \"best_sac_routing_model\"\n",
        "\n",
        "# Initialize Environment and Agent using Optuna results\n",
        "env = RoutingEnv()\n",
        "params = study.best_trial.params\n",
        "\n",
        "agent = SACAgent(\n",
        "    input_dims=env.observation_space.shape[0],\n",
        "    n_actions=env.action_space.n,\n",
        "    lr=params['lr'],\n",
        "    tau=params['tau'],\n",
        "    alpha=params['alpha'],\n",
        "    gamma=params['gamma'],\n",
        "    buffer_capacity=params['buffer_capacity'],\n",
        "    batch_size=params['batch_size'],\n",
        "    fc1_dims=params['fc1_dims'],\n",
        "    fc2_dims=params['fc2_dims']\n",
        ")\n",
        "\n",
        "# Logging Variables\n",
        "total_steps = 0\n",
        "best_avg_eval_reward = -np.inf\n",
        "episode_rewards = deque(maxlen=report_interval)\n",
        "training_losses = deque(maxlen=report_interval)\n",
        "sac_convergence_data = {'episode': [], 'best_eval_reward': []}\n",
        "\n",
        "print(f\"Starting SAC training | Layers: {params['fc1_dims']}/{params['fc2_dims']} | Batch: {params['batch_size']}\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "    step_count = 0\n",
        "    episode_losses = []\n",
        "\n",
        "    while not done and step_count < max_steps:\n",
        "        action = agent.choose_action(state, evaluate=False)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        agent.store_transition(state, action, reward, next_state, done)\n",
        "\n",
        "        if len(agent.replay_buffer) > agent.batch_size * 2:\n",
        "             p_loss, c_loss, _ = agent.learn()\n",
        "             episode_losses.append(p_loss + c_loss)\n",
        "\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "        total_steps += 1\n",
        "        step_count += 1\n",
        "\n",
        "    episode_rewards.append(episode_reward)\n",
        "    if episode_losses:\n",
        "        training_losses.append(np.mean(episode_losses))\n",
        "\n",
        "    # Periodic Progress Report\n",
        "    if (episode + 1) % report_interval == 0:\n",
        "        avg_rew = np.mean(episode_rewards)\n",
        "        avg_loss = np.mean(training_losses) if training_losses else 0.0\n",
        "        print(f\"Ep {episode + 1}/{num_episodes} | Steps: {total_steps} | Avg Reward: {avg_rew:.2f} | Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Evaluation and Model Checkpointing\n",
        "    if (episode + 1) % eval_interval == 0:\n",
        "        eval_rewards = []\n",
        "        eval_rtts = []\n",
        "\n",
        "        for _ in range(num_eval_episodes):\n",
        "            eval_state, _ = env.reset()\n",
        "            eval_done = False\n",
        "            eval_rew = 0\n",
        "            while not eval_done:\n",
        "                eval_action = agent.choose_action(eval_state, evaluate=True)\n",
        "                eval_next, r, term, trunc, info = env.step(eval_action)\n",
        "                eval_done = term or trunc\n",
        "                eval_state = eval_next\n",
        "                eval_rew += r\n",
        "                if eval_done and info.get(\"rtt_latency\"):\n",
        "                    eval_rtts.append(info[\"rtt_latency\"])\n",
        "            eval_rewards.append(eval_rew)\n",
        "\n",
        "        current_avg_reward = np.mean(eval_rewards)\n",
        "        avg_rtt = np.mean(eval_rtts) if eval_rtts else 0\n",
        "\n",
        "        print(f\"--- Eval at Ep {episode + 1} | Avg Reward: {current_avg_reward:.2f} | Avg RTT: {avg_rtt:.2f} ---\")\n",
        "\n",
        "        if current_avg_reward > best_avg_eval_reward:\n",
        "            best_avg_eval_reward = current_avg_reward\n",
        "            agent.save_models(best_model_path)\n",
        "            print(f\"New Best Model Saved ({best_avg_eval_reward:.2f})\")\n",
        "\n",
        "        sac_convergence_data['episode'].append(episode + 1)\n",
        "        sac_convergence_data['best_eval_reward'].append(best_avg_eval_reward)\n",
        "\n",
        "print(f\"\\nTraining Complete | Time: {time.time() - start_time:.2f}s | Best Reward: {best_avg_eval_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu1pb5xfBbRa"
      },
      "source": [
        "# evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zrj7pOBOBZcQ"
      },
      "outputs": [],
      "source": [
        "# --- Test the trained SAC model ---\n",
        "\n",
        "# Instantiate the environment\n",
        "env = RoutingEnv()\n",
        "input_dims = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "# --- Agent Initialization and Model Loading ---\n",
        "try:\n",
        "    # Attempt to use best_sac_params if available\n",
        "    agent_test = SACAgent(input_dims=input_dims, n_actions=n_actions,\n",
        "                          fc1_dims=best_sac_params['fc1_dims'],\n",
        "                          fc2_dims=best_sac_params['fc2_dims'])\n",
        "    print(\"Initialized agent for testing using best_sac_params.\")\n",
        "except NameError:\n",
        "    # Fallback if best_sac_params is not defined\n",
        "    print(\"best_sac_params not found. Initializing agent with default/observed FC dims (512, 128).\")\n",
        "    agent_test = SACAgent(input_dims=input_dims, n_actions=n_actions,\n",
        "                          fc1_dims=512, fc2_dims=128)\n",
        "\n",
        "best_model_path = \"best_sac_routing_model\"\n",
        "\n",
        "try:\n",
        "    agent_test.load_models(best_model_path)\n",
        "    print(f\"Successfully loaded SAC model from: {best_model_path}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Model file not found at {best_model_path}. Please ensure the training was completed and the model was saved.\")\n",
        "    # Exit or handle the error appropriately if the model cannot be loaded\n",
        "    # Re-raising the error instead of calling exit() in a notebook environment\n",
        "    raise FileNotFoundError(f\"Model file not found at {best_model_path}\")\n",
        "\n",
        "# --- Testing Loop Setup ---\n",
        "\n",
        "num_test_episodes = 1000 # Number of episodes to test\n",
        "# Lists to store ALL episode results (NaNs will be present for RTT/Jitter on failures)\n",
        "test_rewards = []\n",
        "test_rtt_latency = []\n",
        "test_throughput_proxy = []\n",
        "test_jitter_std_dev = []\n",
        "test_successful_hops_count = []\n",
        "test_is_success = [] # New list to store boolean success (based on terminal reason)\n",
        "\n",
        "# Use a large safety step limit based on environment's TTL\n",
        "MAX_STEPS_LIMIT = env.max_ttl * 2 + 5\n",
        "\n",
        "print(\"\\nStarting SAC model testing...\")\n",
        "\n",
        "# --- Testing Loop ---\n",
        "\n",
        "for episode in range(num_test_episodes):\n",
        "    state, info = env.reset()\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "    step_count = 0\n",
        "\n",
        "    while not done and step_count < MAX_STEPS_LIMIT:\n",
        "\n",
        "        action = agent_test.choose_action(state, evaluate=True)\n",
        "\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "        step_count += 1\n",
        "\n",
        "    # Store overall episode reward\n",
        "    test_rewards.append(episode_reward)\n",
        "\n",
        "    # Collect additional metrics from the info dictionary\n",
        "    # The info dict is only reliable when terminated=True was set in the last step\n",
        "    if done:\n",
        "        # FIX 1: Determine success based on the correct terminal reason\n",
        "        is_success = info.get('terminal_reason') == \"Goal Reached\"\n",
        "        test_is_success.append(is_success)\n",
        "\n",
        "        # Store all metrics (even from failures, where they might be NaN)\n",
        "        test_successful_hops_count.append(info.get(\"successful_hops\", float('nan')))\n",
        "        test_rtt_latency.append(info.get(\"rtt_latency\", float('nan')))\n",
        "        test_throughput_proxy.append(info.get(\"throughput_proxy\", float('nan')))\n",
        "        test_jitter_std_dev.append(info.get(\"jitter_std_dev\", float('nan')))\n",
        "    else:\n",
        "        # If the loop ended due to MAX_STEPS_LIMIT (truncated externally), treat as failure\n",
        "        test_is_success.append(False)\n",
        "        test_successful_hops_count.append(float('nan'))\n",
        "        test_rtt_latency.append(float('nan'))\n",
        "        test_throughput_proxy.append(float('nan'))\n",
        "        test_jitter_std_dev.append(float('nan'))\n",
        "\n",
        "\n",
        "    # Print episode summary\n",
        "    print(f\"Test Episode {episode + 1}/{num_test_episodes}, Reward: {episode_reward:.2f}, Steps: {step_count}, Success: {test_is_success[-1]}\")\n",
        "\n",
        "# --- Calculate and Report Summary Metrics (Updated) ---\n",
        "\n",
        "avg_test_reward = np.mean(test_rewards)\n",
        "test_success_rate = np.mean(test_is_success) if test_is_success else 0.0\n",
        "\n",
        "# FIX 2: Use the boolean success array to filter all relevant metrics\n",
        "successful_episodes_indices = np.array(test_is_success, dtype=bool)\n",
        "\n",
        "# Convert lists to NumPy arrays for easy filtering\n",
        "test_rtt_latency_np = np.array(test_rtt_latency)\n",
        "test_throughput_proxy_np = np.array(test_throughput_proxy)\n",
        "test_jitter_std_dev_np = np.array(test_jitter_std_dev)\n",
        "test_successful_hops_count_np = np.array(test_successful_hops_count)\n",
        "\n",
        "# Filter metrics for successful episodes only\n",
        "successful_rtt = test_rtt_latency_np[successful_episodes_indices]\n",
        "successful_throughput = test_throughput_proxy_np[successful_episodes_indices]\n",
        "successful_jitter = test_jitter_std_dev_np[successful_episodes_indices]\n",
        "successful_hops = test_successful_hops_count_np[successful_episodes_indices]\n",
        "\n",
        "# Calculate averages (checking if any successful episodes occurred)\n",
        "avg_test_rtt_latency = np.mean(successful_rtt) if len(successful_rtt) > 0 else float('nan')\n",
        "avg_test_throughput_proxy = np.mean(successful_throughput) if len(successful_throughput) > 0 else float('nan')\n",
        "avg_test_jitter_std_dev = np.mean(successful_jitter) if len(successful_jitter) > 0 else float('nan')\n",
        "avg_test_successful_hops_count = np.mean(successful_hops) if len(successful_hops) > 0 else float('nan')\n",
        "\n",
        "\n",
        "print(f\"\\n--- SAC Model Testing Summary over {num_test_episodes} episodes ---\")\n",
        "print(f\"Average Total Reward: {avg_test_reward:.2f}\")\n",
        "print(f\"Success Rate: {test_success_rate:.2%}\")\n",
        "\n",
        "if len(successful_rtt) > 0:\n",
        "    print(f\"Average RTT / Latency (Successful Episodes): {avg_test_rtt_latency:.2f}\")\n",
        "if len(successful_hops) > 0:\n",
        "    print(f\"Average Successful Hops Count (Successful Episodes): {avg_test_successful_hops_count:.2f}\")\n",
        "if len(successful_throughput) > 0:\n",
        "    print(f\"Average Throughput Proxy (Successful Episodes): {avg_test_throughput_proxy:.4f}\")\n",
        "if len(successful_jitter) > 0:\n",
        "    print(f\"Average Jitter (Std Dev of Delays) (Successful Episodes): {avg_test_jitter_std_dev:.4f}\")\n",
        "\n",
        "print(\"SAC model testing finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbaaa5e6"
      },
      "source": [
        "## Encoure RL\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72effec1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class EncoreRLAgent:\n",
        "    def __init__(self, input_dims, n_actions, lr=1e-4, gamma=0.99, epsilon_start=1.0,\n",
        "                 epsilon_end=0.01, epsilon_decay=0.995, target_update=100,\n",
        "                 buffer_capacity=10000, batch_size=64, fc_dims=256):\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.target_update = target_update\n",
        "        self.batch_size = batch_size\n",
        "        self.n_actions = n_actions\n",
        "        self.learn_step_counter = 0\n",
        "\n",
        "        # Network Architectures\n",
        "        self.q_network = nn.Sequential(\n",
        "            nn.Linear(input_dims, fc_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc_dims, fc_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc_dims, n_actions)\n",
        "        )\n",
        "\n",
        "        self.target_q_network = nn.Sequential(\n",
        "            nn.Linear(input_dims, fc_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc_dims, fc_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc_dims, n_actions)\n",
        "        )\n",
        "\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
        "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
        "\n",
        "        self.update_target_network()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_q_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        if random.random() > self.epsilon:\n",
        "            state = torch.tensor(np.array([observation]), dtype=torch.float)\n",
        "            with torch.no_grad():\n",
        "                actions = self.q_network(state)\n",
        "            return torch.argmax(actions).item()\n",
        "        return random.randrange(self.n_actions)\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return None\n",
        "\n",
        "        transitions = self.replay_buffer.sample(self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*transitions)\n",
        "\n",
        "        states = torch.tensor(np.array(states), dtype=torch.float)\n",
        "        actions = torch.tensor(np.array(actions), dtype=torch.long).unsqueeze(-1)\n",
        "        rewards = torch.tensor(np.array(rewards), dtype=torch.float).unsqueeze(-1)\n",
        "        next_states = torch.tensor(np.array(next_states), dtype=torch.float)\n",
        "        dones = torch.tensor(np.array(dones), dtype=torch.float).unsqueeze(-1)\n",
        "\n",
        "        # Bellman Equation update logic\n",
        "        current_q_values = self.q_network(states).gather(1, actions)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.target_q_network(next_states).max(1, keepdim=True)[0]\n",
        "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
        "\n",
        "        loss = F.mse_loss(current_q_values, target_q_values)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.learn_step_counter += 1\n",
        "        if self.learn_step_counter % self.target_update == 0:\n",
        "            self.update_target_network()\n",
        "\n",
        "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
        "        return loss.item()\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.replay_buffer.add((state, action, reward, next_state, done))\n",
        "\n",
        "    def save_models(self, path):\n",
        "        torch.save(self.q_network.state_dict(), f\"{path}_q_network.pth\")\n",
        "        torch.save(self.target_q_network.state_dict(), f\"{path}_target_q_network.pth\")\n",
        "        torch.save(self.optimizer.state_dict(), f\"{path}_optimizer.pth\")\n",
        "\n",
        "    def load_models(self, path):\n",
        "        self.q_network.load_state_dict(torch.load(f\"{path}_q_network.pth\"))\n",
        "        self.target_q_network.load_state_dict(torch.load(f\"{path}_target_q_network.pth\"))\n",
        "        self.optimizer.load_state_dict(torch.load(f\"{path}_optimizer.pth\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3ec4491"
      },
      "source": [
        "## Define encoure rl objective for optuna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1a2c50d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def encore_rl_objective(trial):\n",
        "    \"\"\"\n",
        "    Optuna objective function to optimize Encore RL hyperparameters.\n",
        "    Trains for 500 episodes and returns the mean reward from 50 evaluation episodes.\n",
        "    \"\"\"\n",
        "    # Hyperparameter Search Space\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
        "    gamma = trial.suggest_float(\"gamma\", 0.9, 0.999)\n",
        "    epsilon_decay = trial.suggest_float(\"epsilon_decay\", 0.99, 0.9999)\n",
        "    target_update = trial.suggest_int(\"target_update\", 10, 1000)\n",
        "    buffer_capacity = trial.suggest_int(\"buffer_capacity\", 10000, 1_000_000, step=10000)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128, 256])\n",
        "    fc_dims = trial.suggest_categorical(\"fc_dims\", [64, 128, 256, 512])\n",
        "\n",
        "    env = RoutingEnv()\n",
        "\n",
        "    # Initialize Encore RL Agent\n",
        "    agent = EncoreRLAgent(\n",
        "        input_dims=env.observation_space.shape[0],\n",
        "        n_actions=env.action_space.n,\n",
        "        lr=lr,\n",
        "        gamma=gamma,\n",
        "        epsilon_decay=epsilon_decay,\n",
        "        target_update=target_update,\n",
        "        buffer_capacity=buffer_capacity,\n",
        "        batch_size=batch_size,\n",
        "        fc_dims=fc_dims\n",
        "    )\n",
        "\n",
        "    num_training_episodes = 500\n",
        "    max_steps = 100\n",
        "\n",
        "    print(f\"Trial {trial.number}: Starting Encore RL training with params: {trial.params}\")\n",
        "\n",
        "    # Training Loop\n",
        "    for _ in range(num_training_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        step_count = 0\n",
        "\n",
        "        while not done and step_count < max_steps:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.store_transition(state, action, reward, next_state, done)\n",
        "\n",
        "            if len(agent.replay_buffer) > agent.batch_size:\n",
        "                 agent.learn()\n",
        "\n",
        "            state = next_state\n",
        "            step_count += 1\n",
        "\n",
        "    # Evaluation Loop\n",
        "    num_eval_episodes = 50\n",
        "    eval_rewards = []\n",
        "\n",
        "    print(f\"Trial {trial.number}: Starting Encore RL evaluation...\")\n",
        "\n",
        "    for _ in range(num_eval_episodes):\n",
        "        eval_state, _ = env.reset()\n",
        "        eval_done = False\n",
        "        eval_episode_reward = 0\n",
        "        eval_step_count = 0\n",
        "\n",
        "        # Disable exploration for evaluation\n",
        "        original_epsilon = agent.epsilon\n",
        "        agent.epsilon = 0.0\n",
        "\n",
        "        while not eval_done and eval_step_count < max_steps:\n",
        "            eval_action = agent.choose_action(eval_state)\n",
        "            eval_next, eval_reward, term, trunc, _ = env.step(eval_action)\n",
        "            eval_done = term or trunc\n",
        "            eval_state = eval_next\n",
        "            eval_episode_reward += eval_reward\n",
        "            eval_step_count += 1\n",
        "\n",
        "        eval_rewards.append(eval_episode_reward)\n",
        "        agent.epsilon = original_epsilon\n",
        "\n",
        "    avg_eval_reward = np.mean(eval_rewards)\n",
        "    print(f\"Trial {trial.number} Finished. Average Reward: {avg_eval_reward:.2f}\")\n",
        "\n",
        "    return avg_eval_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ef9db5d"
      },
      "source": [
        "## Run optuna study for encore rl\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdd02e3f"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "\n",
        "# Create the study for Encore RL\n",
        "study_encore = optuna.create_study(direction=\"maximize\")\n",
        "\n",
        "n_trials_encore = 50\n",
        "print(\"Starting Optuna study for Encore RL...\")\n",
        "study_encore.optimize(encore_rl_objective, n_trials=n_trials_encore)\n",
        "print(\"Optuna study for Encore RL finished.\")\n",
        "\n",
        "# Results Summary\n",
        "print(\"\\nBest trial for Encore RL:\")\n",
        "trial_encore = study_encore.best_trial\n",
        "print(f\"  Value: {trial_encore.value}\")\n",
        "print(\"  Best Hyperparameters: \")\n",
        "for key, value in trial_encore.params.items():\n",
        "    print(f\"    {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a31ce17"
      },
      "source": [
        "## Train Encoure RL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef436fe7"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "# --- ENCORE RL TRAINING CONFIGURATION ---\n",
        "num_episodes = 10000\n",
        "max_steps = 100\n",
        "report_interval = 50\n",
        "eval_interval = 100\n",
        "num_eval_episodes = 20\n",
        "best_model_path = \"best_encore_rl_routing_model\"\n",
        "\n",
        "env = RoutingEnv()\n",
        "best_params = study_encore.best_trial.params\n",
        "\n",
        "# Initialize Encore RL Agent\n",
        "agent_encore = EncoreRLAgent(\n",
        "    input_dims=env.observation_space.shape[0],\n",
        "    n_actions=env.action_space.n,\n",
        "    lr=best_params['lr'],\n",
        "    gamma=best_params['gamma'],\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_end=0.01,\n",
        "    epsilon_decay=best_params['epsilon_decay'],\n",
        "    target_update=best_params['target_update'],\n",
        "    buffer_capacity=best_params['buffer_capacity'],\n",
        "    batch_size=best_params['batch_size'],\n",
        "    fc_dims=best_params['fc_dims']\n",
        ")\n",
        "\n",
        "# Logging Variables\n",
        "total_steps = 0\n",
        "best_avg_eval_reward = -np.inf\n",
        "episode_rewards = deque(maxlen=report_interval)\n",
        "encore_convergence_data = {'episode': [], 'best_eval_reward': []}\n",
        "\n",
        "print(f\"Starting Encore RL training | Best Params: {best_params}\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "    step_count = 0\n",
        "\n",
        "    while not done and step_count < max_steps:\n",
        "        action = agent_encore.choose_action(state)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        agent_encore.store_transition(state, action, reward, next_state, done)\n",
        "\n",
        "        if len(agent_encore.replay_buffer) > agent_encore.batch_size * 2:\n",
        "             agent_encore.learn()\n",
        "\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "        total_steps += 1\n",
        "        step_count += 1\n",
        "\n",
        "    episode_rewards.append(episode_reward)\n",
        "\n",
        "    # Periodic Progress Report\n",
        "    if (episode + 1) % report_interval == 0:\n",
        "        avg_rew = np.mean(episode_rewards)\n",
        "        print(f\"Ep {episode + 1}/{num_episodes} | Total Steps: {total_steps} | Avg Reward: {avg_rew:.2f} | Epsilon: {agent_encore.epsilon:.4f}\")\n",
        "\n",
        "    # Evaluation and Model Checkpointing\n",
        "    if (episode + 1) % eval_interval == 0:\n",
        "        eval_rewards = []\n",
        "        eval_rtts = []\n",
        "\n",
        "        # Greedy Evaluation\n",
        "        original_epsilon = agent_encore.epsilon\n",
        "        agent_encore.epsilon = 0.0\n",
        "\n",
        "        for _ in range(num_eval_episodes):\n",
        "            eval_state, _ = env.reset()\n",
        "            eval_done = False\n",
        "            eval_rew = 0\n",
        "            while not eval_done:\n",
        "                eval_action = agent_encore.choose_action(eval_state)\n",
        "                eval_next, r, term, trunc, info = env.step(eval_action)\n",
        "                eval_done = term or trunc\n",
        "                eval_state = eval_next\n",
        "                eval_rew += r\n",
        "                if eval_done and info.get(\"rtt_latency\"):\n",
        "                    eval_rtts.append(info[\"rtt_latency\"])\n",
        "            eval_rewards.append(eval_rew)\n",
        "\n",
        "        agent_encore.epsilon = original_epsilon\n",
        "        current_avg_reward = np.mean(eval_rewards)\n",
        "        avg_rtt = np.mean(eval_rtts) if eval_rtts else 0\n",
        "\n",
        "        print(f\"--- Eval at Ep {episode + 1} | Avg Reward: {current_avg_reward:.2f} | Avg RTT: {avg_rtt:.2f} ---\")\n",
        "\n",
        "        if current_avg_reward > best_avg_eval_reward:\n",
        "            best_avg_eval_reward = current_avg_reward\n",
        "            agent_encore.save_models(best_model_path)\n",
        "            print(f\"New Best Encore RL Model Saved ({best_avg_eval_reward:.2f})\")\n",
        "\n",
        "        encore_convergence_data['episode'].append(episode + 1)\n",
        "        encore_convergence_data['best_eval_reward'].append(best_avg_eval_reward)\n",
        "\n",
        "print(f\"\\nEncore RL Training Complete | Time: {time.time() - start_time:.2f}s | Best Reward: {best_avg_eval_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIs3wAazj1CY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- ENCORE RL TEST CONFIGURATION ---\n",
        "num_test_episodes = 1000\n",
        "best_model_path = \"best_encore_rl_routing_model\"\n",
        "env = RoutingEnv()\n",
        "\n",
        "# Initialize Agent for Testing\n",
        "try:\n",
        "    params = study_encore.best_trial.params\n",
        "    agent_test = EncoreRLAgent(\n",
        "        input_dims=env.observation_space.shape[0],\n",
        "        n_actions=env.action_space.n,\n",
        "        epsilon_start=0.0,  # Greedy testing\n",
        "        epsilon_end=0.0,\n",
        "        fc_dims=params['fc_dims']\n",
        "    )\n",
        "    print(\"Initialized Encore RL agent using best_params.\")\n",
        "except NameError:\n",
        "    print(\"best_params not found. Using default FC dims (128).\")\n",
        "    agent_test = EncoreRLAgent(\n",
        "        input_dims=env.observation_space.shape[0],\n",
        "        n_actions=env.action_space.n,\n",
        "        epsilon_start=0.0,\n",
        "        fc_dims=128\n",
        "    )\n",
        "\n",
        "# Load Trained Weights\n",
        "agent_test.load_models(best_model_path)\n",
        "print(f\"Loaded Encore RL model from: {best_model_path}\")\n",
        "\n",
        "# Metric Storage\n",
        "metrics = {\n",
        "    \"rewards\": [],\n",
        "    \"rtt\": [],\n",
        "    \"throughput\": [],\n",
        "    \"jitter\": [],\n",
        "    \"hops\": [],\n",
        "    \"success_flags\": []\n",
        "}\n",
        "\n",
        "max_steps = env.max_ttl * 2 + 5\n",
        "\n",
        "print(f\"\\nStarting Encore RL Testing ({num_test_episodes} episodes)...\")\n",
        "\n",
        "for episode in range(num_test_episodes):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "    step_count = 0\n",
        "\n",
        "    while not done and step_count < max_steps:\n",
        "        action = agent_test.choose_action(state)\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "        step_count += 1\n",
        "\n",
        "    # Log results\n",
        "    is_success = info.get('terminal_reason') == \"Goal Reached\" if done else False\n",
        "    metrics[\"rewards\"].append(episode_reward)\n",
        "    metrics[\"success_flags\"].append(is_success)\n",
        "\n",
        "    # Store per-episode metrics (filtered later for success)\n",
        "    metrics[\"hops\"].append(info.get(\"successful_hops\", np.nan))\n",
        "    metrics[\"rtt\"].append(info.get(\"rtt_latency\", np.nan))\n",
        "    metrics[\"throughput\"].append(info.get(\"throughput_proxy\", np.nan))\n",
        "    metrics[\"jitter\"].append(info.get(\"jitter_std_dev\", np.nan))\n",
        "\n",
        "    if (episode + 1) % 100 == 0:\n",
        "        print(f\"Episode {episode + 1}/{num_test_episodes} | Success: {is_success}\")\n",
        "\n",
        "# --- FINAL SUMMARY REPORT ---\n",
        "success_indices = np.array(metrics[\"success_flags\"], dtype=bool)\n",
        "avg_reward = np.mean(metrics[\"rewards\"])\n",
        "success_rate = np.mean(metrics[\"success_flags\"])\n",
        "\n",
        "print(f\"\\n--- Encore RL Testing Summary ---\")\n",
        "print(f\"Average Total Reward: {avg_reward:.2f}\")\n",
        "print(f\"Success Rate: {success_rate:.2%}\")\n",
        "\n",
        "if any(success_indices):\n",
        "    print(f\"Average RTT (Successful): {np.nanmean(np.array(metrics['rtt'])[success_indices]):.2f}\")\n",
        "    print(f\"Average Hops (Successful): {np.nanmean(np.array(metrics['hops'])[success_indices]):.2f}\")\n",
        "    print(f\"Average Throughput Proxy (Successful): {np.nanmean(np.array(metrics['throughput'])[success_indices]):.4f}\")\n",
        "    print(f\"Average Jitter (Successful): {np.nanmean(np.array(metrics['jitter'])[success_indices]):.4f}\")\n",
        "else:\n",
        "    print(\"No successful episodes recorded.\")\n",
        "\n",
        "print(\"Encore RL testing finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a8b0300"
      },
      "source": [
        "#Define PPO Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fa8a87a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, input_dims, n_actions, fc1_dims=256, fc2_dims=256):\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dims, fc1_dims)\n",
        "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
        "        self.policy = nn.Linear(fc2_dims, n_actions)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.policy(x)\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, input_dims, fc1_dims=256, fc2_dims=256):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dims, fc1_dims)\n",
        "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
        "        self.value = nn.Linear(fc2_dims, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.value(x)\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(self, input_dims, n_actions, lr=3e-4, gamma=0.99, clip_epsilon=0.2,\n",
        "                 ppo_epochs=10, minibatch_size=64, gae_lambda=0.95, fc1_dims=256, fc2_dims=256,\n",
        "                 max_grad_norm=0.5):\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.clip_epsilon = clip_epsilon\n",
        "        self.ppo_epochs = ppo_epochs\n",
        "        self.minibatch_size = minibatch_size\n",
        "        self.gae_lambda = gae_lambda\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "\n",
        "        self.actor = Actor(input_dims, n_actions, fc1_dims, fc2_dims)\n",
        "        self.critic = Critic(input_dims, fc1_dims, fc2_dims)\n",
        "\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)\n",
        "\n",
        "        self.memory = []\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done, log_prob):\n",
        "        self.memory.append((state, action, reward, next_state, done, log_prob))\n",
        "\n",
        "    def clear_memory(self):\n",
        "        self.memory = []\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        state = torch.tensor(np.array([observation]), dtype=torch.float)\n",
        "        with torch.no_grad():\n",
        "            logits = self.actor(state)\n",
        "            dist = Categorical(logits=logits)\n",
        "            action = dist.sample()\n",
        "        return action.item(), dist.log_prob(action)\n",
        "\n",
        "    def learn(self):\n",
        "        if not self.memory:\n",
        "            return None, None\n",
        "\n",
        "        states, actions, rewards, next_states, dones, old_log_probs = zip(*self.memory)\n",
        "\n",
        "        states = torch.tensor(np.array(states), dtype=torch.float)\n",
        "        actions = torch.tensor(np.array(actions), dtype=torch.long)\n",
        "        rewards = torch.tensor(np.array(rewards), dtype=torch.float)\n",
        "        next_states = torch.tensor(np.array(next_states), dtype=torch.float)\n",
        "        dones = torch.tensor(np.array(dones), dtype=torch.float)\n",
        "        old_log_probs = torch.tensor(np.array(old_log_probs), dtype=torch.float)\n",
        "\n",
        "        # GAE Calculation\n",
        "        with torch.no_grad():\n",
        "            values = self.critic(states).squeeze(-1)\n",
        "            next_values = self.critic(next_states).squeeze(-1)\n",
        "\n",
        "        advantages = torch.zeros_like(rewards)\n",
        "        last_gae_lambda = 0\n",
        "\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            next_val = next_values[t] if t == len(rewards) - 1 else values[t+1]\n",
        "            delta = rewards[t] + self.gamma * next_val * (1 - dones[t]) - values[t]\n",
        "            advantages[t] = last_gae_lambda = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * last_gae_lambda\n",
        "\n",
        "        returns = advantages + values\n",
        "\n",
        "        num_samples = len(states)\n",
        "        indices = np.arange(num_samples)\n",
        "        avg_actor_loss, avg_critic_loss = 0, 0\n",
        "\n",
        "        for _ in range(self.ppo_epochs):\n",
        "            np.random.shuffle(indices)\n",
        "            for start_idx in range(0, num_samples, self.minibatch_size):\n",
        "                idx = indices[start_idx:start_idx + self.minibatch_size]\n",
        "\n",
        "                # Advantage Normalization\n",
        "                batch_adv = advantages[idx]\n",
        "                if len(batch_adv) > 1:\n",
        "                    batch_adv = (batch_adv - batch_adv.mean()) / (batch_adv.std() + 1e-8)\n",
        "\n",
        "                # Update Critic\n",
        "                new_values = self.critic(states[idx]).squeeze(-1)\n",
        "                critic_loss = F.mse_loss(new_values, returns[idx])\n",
        "\n",
        "                # Update Actor\n",
        "                dist = Categorical(logits=self.actor(states[idx]))\n",
        "                new_log_probs = dist.log_prob(actions[idx])\n",
        "                ratio = torch.exp(new_log_probs - old_log_probs[idx])\n",
        "\n",
        "                surr1 = ratio * batch_adv\n",
        "                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_adv\n",
        "                actor_loss = -torch.min(surr1, surr2).mean() - 0.01 * dist.entropy().mean()\n",
        "\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "                self.critic_optimizer.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)\n",
        "                self.critic_optimizer.step()\n",
        "\n",
        "                avg_actor_loss += actor_loss.item()\n",
        "                avg_critic_loss += critic_loss.item()\n",
        "\n",
        "        self.clear_memory()\n",
        "        total_updates = self.ppo_epochs * (num_samples // self.minibatch_size)\n",
        "        return avg_actor_loss / total_updates, avg_critic_loss / total_updates\n",
        "\n",
        "    def save_models(self, path):\n",
        "        torch.save(self.actor.state_dict(), f\"{path}_actor.pth\")\n",
        "        torch.save(self.critic.state_dict(), f\"{path}_critic.pth\")\n",
        "\n",
        "    def load_models(self, path):\n",
        "        self.actor.load_state_dict(torch.load(f\"{path}_actor.pth\"))\n",
        "        self.critic.load_state_dict(torch.load(f\"{path}_critic.pth\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7s1xejZbp5i"
      },
      "source": [
        "#Optuna obj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f10dd7f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def ppo_objective(trial):\n",
        "    \"\"\"\n",
        "    Optuna objective function to optimize PPO hyperparameters.\n",
        "    Trains for 500 episodes and returns the average reward from 50 evaluation episodes.\n",
        "    \"\"\"\n",
        "    # Hyperparameter Search Space\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
        "    gamma = trial.suggest_float(\"gamma\", 0.9, 0.999)\n",
        "    clip_epsilon = trial.suggest_float(\"clip_epsilon\", 0.1, 0.3)\n",
        "    ppo_epochs = trial.suggest_int(\"ppo_epochs\", 5, 20)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128, 256])\n",
        "    gae_lambda = trial.suggest_float(\"gae_lambda\", 0.9, 0.99)\n",
        "    fc1_dims = trial.suggest_categorical(\"fc1_dims\", [64, 128, 256, 512])\n",
        "    fc2_dims = trial.suggest_categorical(\"fc2_dims\", [64, 128, 256, 512])\n",
        "\n",
        "    env = RoutingEnv()\n",
        "\n",
        "    agent = PPOAgent(\n",
        "        input_dims=env.observation_space.shape[0],\n",
        "        n_actions=env.action_space.n,\n",
        "        lr=lr,\n",
        "        gamma=gamma,\n",
        "        clip_epsilon=clip_epsilon,\n",
        "        ppo_epochs=ppo_epochs,\n",
        "        minibatch_size=batch_size,\n",
        "        gae_lambda=gae_lambda,\n",
        "        fc1_dims=fc1_dims,\n",
        "        fc2_dims=fc2_dims\n",
        "    )\n",
        "\n",
        "    num_training_episodes = 500\n",
        "    max_steps = 100\n",
        "\n",
        "    print(f\"Trial {trial.number}: Starting PPO training with params: {trial.params}\")\n",
        "\n",
        "    # Training Loop\n",
        "    for _ in range(num_training_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        step_count = 0\n",
        "        agent.clear_memory()\n",
        "\n",
        "        while not done and step_count < max_steps:\n",
        "            action, log_prob = agent.choose_action(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.store_transition(state, action, reward, next_state, done, log_prob)\n",
        "\n",
        "            state = next_state\n",
        "            step_count += 1\n",
        "\n",
        "        if len(agent.memory) > 0:\n",
        "             agent.learn()\n",
        "\n",
        "    # Evaluation Loop\n",
        "    num_eval_episodes = 50\n",
        "    eval_rewards = []\n",
        "\n",
        "    print(f\"Trial {trial.number}: Starting PPO evaluation...\")\n",
        "\n",
        "    for _ in range(num_eval_episodes):\n",
        "        eval_state, _ = env.reset()\n",
        "        eval_done = False\n",
        "        eval_episode_reward = 0\n",
        "        eval_step_count = 0\n",
        "\n",
        "        while not eval_done and eval_step_count < max_steps:\n",
        "            # Greedy action selection\n",
        "            with torch.no_grad():\n",
        "                logits = agent.actor(torch.tensor(np.array([eval_state]), dtype=torch.float))\n",
        "                eval_action = torch.argmax(logits).item()\n",
        "\n",
        "            eval_next, eval_reward, term, trunc, _ = env.step(eval_action)\n",
        "            eval_done = term or trunc\n",
        "            eval_state = eval_next\n",
        "            eval_episode_reward += eval_reward\n",
        "            eval_step_count += 1\n",
        "\n",
        "        eval_rewards.append(eval_episode_reward)\n",
        "\n",
        "    avg_eval_reward = np.mean(eval_rewards)\n",
        "    print(f\"Trial {trial.number} Finished. Average Reward: {avg_eval_reward:.2f}\")\n",
        "\n",
        "    return avg_eval_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adce1610"
      },
      "source": [
        "## Run Optuna Study for PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa0557d4"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "\n",
        "# Create the study for PPO\n",
        "study_ppo = optuna.create_study(direction=\"maximize\")\n",
        "\n",
        "n_trials_ppo = 50\n",
        "print(\"Starting Optuna study for PPO...\")\n",
        "study_ppo.optimize(ppo_objective, n_trials=n_trials_ppo)\n",
        "print(\"Optuna study for PPO finished.\")\n",
        "\n",
        "# Results Summary\n",
        "print(\"\\nBest trial for PPO:\")\n",
        "trial_ppo = study_ppo.best_trial\n",
        "print(f\"  Value: {trial_ppo.value}\")\n",
        "print(\"  Best Hyperparameters: \")\n",
        "for key, value in trial_ppo.params.items():\n",
        "    print(f\"    {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f6810dd"
      },
      "source": [
        "## Train PPO with Best Hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93b28127"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "# --- PPO TRAINING CONFIGURATION ---\n",
        "num_episodes = 10000\n",
        "max_steps = 100\n",
        "ROLLOUT_BUFFER_SIZE = 2048\n",
        "report_interval = 50\n",
        "eval_interval = 100\n",
        "num_eval_episodes = 20\n",
        "best_model_path = \"best_ppo_routing_model\"\n",
        "\n",
        "env = RoutingEnv()\n",
        "\n",
        "# Load hyperparameters from Optuna or use defaults\n",
        "try:\n",
        "    params = study_ppo.best_trial.params\n",
        "except NameError:\n",
        "    params = {\n",
        "        'lr': 3e-4, 'gamma': 0.99, 'clip_epsilon': 0.2, 'ppo_epochs': 10,\n",
        "        'batch_size': 64, 'gae_lambda': 0.95, 'fc1_dims': 256, 'fc2_dims': 256\n",
        "    }\n",
        "\n",
        "agent_ppo = PPOAgent(\n",
        "    input_dims=env.observation_space.shape[0],\n",
        "    n_actions=env.action_space.n,\n",
        "    lr=params['lr'],\n",
        "    gamma=params['gamma'],\n",
        "    clip_epsilon=params['clip_epsilon'],\n",
        "    ppo_epochs=params['ppo_epochs'],\n",
        "    minibatch_size=params['batch_size'],\n",
        "    gae_lambda=params['gae_lambda'],\n",
        "    fc1_dims=params['fc1_dims'],\n",
        "    fc2_dims=params['fc2_dims']\n",
        ")\n",
        "\n",
        "# Tracking Variables\n",
        "total_steps = 0\n",
        "best_avg_eval_reward = -np.inf\n",
        "episode_rewards = deque(maxlen=report_interval)\n",
        "actor_losses, critic_losses = [], []\n",
        "\n",
        "print(f\"Starting PPO training | Buffer Size: {ROLLOUT_BUFFER_SIZE} | Batch: {params['batch_size']}\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "    step_count = 0\n",
        "\n",
        "    while not done and step_count < max_steps:\n",
        "        action, log_prob = agent_ppo.choose_action(state)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        agent_ppo.store_transition(state, action, reward, next_state, done, log_prob)\n",
        "\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "        total_steps += 1\n",
        "        step_count += 1\n",
        "\n",
        "        # Update policy only after the rollout buffer is full\n",
        "        if len(agent_ppo.memory) >= ROLLOUT_BUFFER_SIZE:\n",
        "            a_loss, c_loss = agent_ppo.learn()\n",
        "            actor_losses.append(a_loss)\n",
        "            critic_losses.append(c_loss)\n",
        "\n",
        "    episode_rewards.append(episode_reward)\n",
        "\n",
        "    # Periodic Progress Report\n",
        "    if (episode + 1) % report_interval == 0:\n",
        "        avg_rew = np.mean(episode_rewards)\n",
        "        curr_al = actor_losses[-1] if actor_losses else 0.0\n",
        "        print(f\"Ep {episode+1}/{num_episodes} | Steps: {total_steps} | Avg Reward: {avg_rew:.2f} | Actor Loss: {curr_al:.4f}\")\n",
        "\n",
        "    # Evaluation and Model Checkpointing\n",
        "    if (episode + 1) % eval_interval == 0:\n",
        "        eval_rewards = []\n",
        "        for _ in range(num_eval_episodes):\n",
        "            eval_state, _ = env.reset()\n",
        "            eval_done, eval_rew = False, 0\n",
        "            while not eval_done:\n",
        "                with torch.no_grad():\n",
        "                    logits = agent_ppo.actor(torch.tensor(np.array([eval_state]), dtype=torch.float))\n",
        "                    eval_action = torch.argmax(logits).item()\n",
        "                eval_next, r, term, trunc, _ = env.step(eval_action)\n",
        "                eval_done = term or trunc\n",
        "                eval_state = eval_next\n",
        "                eval_rew += r\n",
        "            eval_rewards.append(eval_rew)\n",
        "\n",
        "        current_avg_reward = np.mean(eval_rewards)\n",
        "        print(f\"--- Eval at Ep {episode + 1} | Avg Reward: {current_avg_reward:.2f} ---\")\n",
        "\n",
        "        if current_avg_reward > best_avg_eval_reward:\n",
        "            best_avg_eval_reward = current_avg_reward\n",
        "            agent_ppo.save_models(best_model_path)\n",
        "            print(f\"New Best PPO Model Saved ({best_avg_eval_reward:.2f})\")\n",
        "\n",
        "print(f\"\\nPPO Training Complete | Time: {time.time() - start_time:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1aec0eb"
      },
      "source": [
        "## Evaluate PPO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcPawF1N-abh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# --- PPO TEST CONFIGURATION ---\n",
        "num_test_episodes = 1000\n",
        "best_model_path = \"best_ppo_routing_model\"\n",
        "env = RoutingEnv()\n",
        "\n",
        "# Initialize Agent for Testing\n",
        "try:\n",
        "    params = study_ppo.best_trial.params\n",
        "    agent_test = PPOAgent(\n",
        "        input_dims=env.observation_space.shape[0],\n",
        "        n_actions=env.action_space.n,\n",
        "        fc1_dims=params['fc1_dims'],\n",
        "        fc2_dims=params['fc2_dims']\n",
        "    )\n",
        "    print(\"Initialized PPO agent using best_params.\")\n",
        "except NameError:\n",
        "    print(\"best_params not found. Using default FC dims (512, 128).\")\n",
        "    agent_test = PPOAgent(\n",
        "        input_dims=env.observation_space.shape[0],\n",
        "        n_actions=env.action_space.n,\n",
        "        fc1_dims=512, fc2_dims=128\n",
        "    )\n",
        "\n",
        "# Load Trained Weights\n",
        "agent_test.load_models(best_model_path)\n",
        "print(f\"Loaded PPO model from: {best_model_path}\")\n",
        "\n",
        "# Metric Storage\n",
        "metrics = {\n",
        "    \"rewards\": [],\n",
        "    \"rtt\": [],\n",
        "    \"throughput\": [],\n",
        "    \"jitter\": [],\n",
        "    \"hops\": [],\n",
        "    \"success_flags\": []\n",
        "}\n",
        "\n",
        "max_steps = env.max_ttl * 2 + 5\n",
        "\n",
        "print(f\"\\nStarting PPO Testing ({num_test_episodes} episodes)...\")\n",
        "\n",
        "for episode in range(num_test_episodes):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "    step_count = 0\n",
        "\n",
        "    while not done and step_count < max_steps:\n",
        "        # Greedy inference\n",
        "        with torch.no_grad():\n",
        "            logits = agent_test.actor(torch.tensor(np.array([state]), dtype=torch.float))\n",
        "            action = torch.argmax(logits).item()\n",
        "\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "        step_count += 1\n",
        "\n",
        "    # Log results\n",
        "    is_success = info.get('terminal_reason') == \"Goal Reached\" if done else False\n",
        "    metrics[\"rewards\"].append(episode_reward)\n",
        "    metrics[\"success_flags\"].append(is_success)\n",
        "\n",
        "    # Store per-episode metrics (will be filtered for success in summary)\n",
        "    metrics[\"hops\"].append(info.get(\"successful_hops\", np.nan))\n",
        "    metrics[\"rtt\"].append(info.get(\"rtt_latency\", np.nan))\n",
        "    metrics[\"throughput\"].append(info.get(\"throughput_proxy\", np.nan))\n",
        "    metrics[\"jitter\"].append(info.get(\"jitter_std_dev\", np.nan))\n",
        "\n",
        "    if (episode + 1) % 100 == 0:\n",
        "        print(f\"Episode {episode + 1}/{num_test_episodes} | Success: {is_success}\")\n",
        "\n",
        "# --- FINAL SUMMARY REPORT ---\n",
        "success_indices = np.array(metrics[\"success_flags\"], dtype=bool)\n",
        "avg_reward = np.mean(metrics[\"rewards\"])\n",
        "success_rate = np.mean(metrics[\"success_flags\"])\n",
        "\n",
        "print(f\"\\n--- PPO Model Testing Summary ---\")\n",
        "print(f\"Average Total Reward: {avg_reward:.2f}\")\n",
        "print(f\"Success Rate: {success_rate:.2%}\")\n",
        "\n",
        "if any(success_indices):\n",
        "    print(f\"Average RTT (Successful): {np.nanmean(np.array(metrics['rtt'])[success_indices]):.2f}\")\n",
        "    print(f\"Average Hops (Successful): {np.nanmean(np.array(metrics['hops'])[success_indices]):.2f}\")\n",
        "    print(f\"Average Throughput Proxy (Successful): {np.nanmean(np.array(metrics['throughput'])[success_indices]):.4f}\")\n",
        "    print(f\"Average Jitter (Successful): {np.nanmean(np.array(metrics['jitter'])[success_indices]):.4f}\")\n",
        "else:\n",
        "    print(\"No successful episodes recorded.\")\n",
        "\n",
        "print(\"PPO model testing finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec5e165d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- CONVERGENCE VISUALIZATION ---\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot SAC Data\n",
        "if sac_convergence_data.get('episode'):\n",
        "    plt.plot(sac_convergence_data['episode'],\n",
        "             sac_convergence_data['best_eval_reward'],\n",
        "             label='SAC', linewidth=2)\n",
        "\n",
        "# Plot Encore RL Data\n",
        "if encore_convergence_data.get('episode'):\n",
        "    plt.plot(encore_convergence_data['episode'],\n",
        "             encore_convergence_data['best_eval_reward'],\n",
        "             label='Encore RL', linewidth=2)\n",
        "\n",
        "# Plot PPO Data\n",
        "if ppo_convergence_data.get('episode'):\n",
        "    plt.plot(ppo_convergence_data['episode'],\n",
        "             ppo_convergence_data['best_eval_reward'],\n",
        "             label='PPO', linewidth=2)\n",
        "\n",
        "# Plot Styling\n",
        "plt.xlabel('Episode', fontsize=12)\n",
        "plt.ylabel('Best Average Evaluation Reward', fontsize=12)\n",
        "plt.title('Agent Convergence Comparison', fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fef0c7b3"
      },
      "source": [
        "# GPSR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fdf1178"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- GPSR BASELINE SETUP ---\n",
        "\n",
        "node_coordinates = {\n",
        "    0: (0, 50),    # Source\n",
        "    1: (25, 75),   # Top Path\n",
        "    2: (25, 25),   # Bottom Path\n",
        "    3: (50, 50),   # Middle Hop\n",
        "    4: (100, 50)   # Destination\n",
        "}\n",
        "\n",
        "def gpsr_choose_action(current_node, destination_node, topology, coords):\n",
        "    \"\"\"\n",
        "    Standard GPSR Greedy Forwarding: Selects the neighbor strictly closest\n",
        "    to the destination coordinates.\n",
        "    \"\"\"\n",
        "    if current_node == destination_node or destination_node not in coords:\n",
        "        return 'drop'\n",
        "\n",
        "    dest_pos = coords[destination_node]\n",
        "\n",
        "    def get_dist(n_id):\n",
        "        curr_pos = coords[n_id]\n",
        "        return np.sqrt((curr_pos[0] - dest_pos[0])**2 + (curr_pos[1] - dest_pos[1])**2)\n",
        "\n",
        "    my_dist = get_dist(current_node)\n",
        "    best_neighbor = 'drop'\n",
        "    min_dist = my_dist\n",
        "\n",
        "    for neighbor in topology.get(current_node, []):\n",
        "        d = get_dist(neighbor)\n",
        "        if d < min_dist:\n",
        "            min_dist = d\n",
        "            best_neighbor = neighbor\n",
        "\n",
        "    return best_neighbor\n",
        "\n",
        "# --- GPSR TESTING LOOP ---\n",
        "\n",
        "env = RoutingEnv()\n",
        "num_test_episodes = 1000\n",
        "max_steps = env.max_ttl * 2 + 5\n",
        "\n",
        "metrics = {\n",
        "    \"rewards\": [],\n",
        "    \"success_flags\": [],\n",
        "    \"rtt\": [],\n",
        "    \"hops\": []\n",
        "}\n",
        "\n",
        "print(f\"Starting GPSR Testing ({num_test_episodes} episodes)...\")\n",
        "\n",
        "for episode in range(num_test_episodes):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "    step_count = 0\n",
        "\n",
        "    # Derive IDs from One-Hot state vector\n",
        "    current_node = np.argmax(state[:env.total_nodes])\n",
        "    destination_node = np.argmax(state[env.total_nodes:2*env.total_nodes])\n",
        "\n",
        "    while not done and step_count < max_steps:\n",
        "        # GPSR Decision\n",
        "        chosen_node = gpsr_choose_action(current_node, destination_node, env.network_topology, node_coordinates)\n",
        "\n",
        "        # Map Node ID to Env Action Index\n",
        "        actions = env.get_action_space(current_node)\n",
        "        action_idx = actions.index(chosen_node) if chosen_node in actions else len(actions)\n",
        "\n",
        "        next_state, reward, terminated, truncated, info = env.step(action_idx)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if info.get('link_successful'):\n",
        "             current_node = np.argmax(next_state[:env.total_nodes])\n",
        "\n",
        "        episode_reward += reward\n",
        "        step_count += 1\n",
        "\n",
        "    # Store results\n",
        "    is_success = info.get('terminal_reason') == \"Goal Reached\" if done else False\n",
        "    metrics[\"rewards\"].append(episode_reward)\n",
        "    metrics[\"success_flags\"].append(is_success)\n",
        "    metrics[\"rtt\"].append(info.get(\"rtt_latency\", np.nan))\n",
        "    metrics[\"hops\"].append(info.get(\"successful_hops\", np.nan))\n",
        "\n",
        "# --- SUMMARY REPORT ---\n",
        "success_indices = np.array(metrics[\"success_flags\"], dtype=bool)\n",
        "avg_reward = np.mean(metrics[\"rewards\"])\n",
        "success_rate = np.mean(metrics[\"success_flags\"])\n",
        "\n",
        "print(f\"\\n--- GPSR Baseline Summary ---\")\n",
        "print(f\"Average Reward: {avg_reward:.2f}\")\n",
        "print(f\"Success Rate: {success_rate:.2%}\")\n",
        "\n",
        "if any(success_indices):\n",
        "    print(f\"Avg RTT (Successful): {np.nanmean(np.array(metrics['rtt'])[success_indices]):.2f}\")\n",
        "    print(f\"Avg Hops (Successful): {np.nanmean(np.array(metrics['hops'])[success_indices]):.2f}\")\n",
        "\n",
        "print(\"GPSR testing finished.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d30662be"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "def aggregate_metrics(agent_name, metrics_dict):\n",
        "    \"\"\"\n",
        "    Formats raw agent metrics into a DataFrame-friendly list.\n",
        "    Filters RTT and Throughput for successful episodes only.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    success_flags = np.array(metrics_dict.get('success_flags', []), dtype=bool)\n",
        "\n",
        "    # 1. RTT and Throughput (Successful Episodes Only)\n",
        "    if 'rtt' in metrics_dict:\n",
        "        rtts = np.array(metrics_dict['rtt'])[success_flags]\n",
        "        data.extend([{'Agent': agent_name, 'Metric': 'RTT / Latency', 'Value': v} for v in rtts if not np.isnan(v)])\n",
        "\n",
        "    if 'throughput' in metrics_dict:\n",
        "        tps = np.array(metrics_dict['throughput'])[success_flags]\n",
        "        data.extend([{'Agent': agent_name, 'Metric': 'Throughput', 'Value': v} for v in tps if not np.isnan(v)])\n",
        "\n",
        "    # 2. Success Rate (Calculated as a single percentage point for the bar chart)\n",
        "    if len(success_flags) > 0:\n",
        "        rate = np.mean(success_flags) * 100\n",
        "        data.append({'Agent': agent_name, 'Metric': 'Success Rate', 'Value': rate})\n",
        "\n",
        "    # 3. Total Rewards (All Episodes)\n",
        "    if 'rewards' in metrics_dict:\n",
        "        data.extend([{'Agent': agent_name, 'Metric': 'Total Reward', 'Value': v} for v in metrics_dict['rewards']])\n",
        "\n",
        "    return data\n",
        "\n",
        "# --- Data Collection ---\n",
        "all_plot_data = []\n",
        "\n",
        "# Aggregate data for all agents (assuming metrics dictionaries are available)\n",
        "if 'sac_metrics' in locals():\n",
        "    all_plot_data.extend(aggregate_metrics('SAC', sac_metrics))\n",
        "\n",
        "if 'encore_rl_metrics' in locals():\n",
        "    all_plot_data.extend(aggregate_metrics('Encore RL', encore_rl_metrics))\n",
        "\n",
        "if 'ppo_metrics' in locals():\n",
        "    all_plot_data.extend(aggregate_metrics('PPO', ppo_metrics))\n",
        "\n",
        "if 'gpsr_metrics' in locals():\n",
        "    all_plot_data.extend(aggregate_metrics('GPSR', gpsr_metrics))\n",
        "\n",
        "df_plot = pd.DataFrame(all_plot_data)\n",
        "\n",
        "# --- Plotting ---\n",
        "metrics_to_show = ['Throughput', 'RTT / Latency', 'Success Rate', 'Total Reward']\n",
        "fig, axes = plt.subplots(1, 4, figsize=(20, 6))\n",
        "\n",
        "for i, metric in enumerate(metrics_to_show):\n",
        "    subset = df_plot[df_plot['Metric'] == metric]\n",
        "\n",
        "    if metric == 'Success Rate':\n",
        "        # Bar plot for categorical comparison\n",
        "        sns.barplot(x='Agent', y='Value', data=subset, ax=axes[i], palette='viridis')\n",
        "        axes[i].set_ylabel('Success Rate (%)')\n",
        "        axes[i].set_ylim(0, 100)\n",
        "    else:\n",
        "        # Violin plot for distribution comparison\n",
        "        sns.violinplot(x='Agent', y='Value', data=subset, ax=axes[i], inner='box', palette='viridis')\n",
        "        axes[i].set_ylabel(f'{metric} Distribution')\n",
        "\n",
        "    axes[i].set_title(f'Agent {metric}')\n",
        "    axes[i].set_xlabel('Agent')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pp8b2qHLUPst"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}